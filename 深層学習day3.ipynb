{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section1: 再帰型ニューラルネットワークの概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）は音声データやテキストデータなどの時系列データをに対応可能なニューラルネットワークである。RNNでは、通常のニューラルネットワークの中間層の出力を出力層へ渡すだけでなく、次の時点の中間層へ渡すことによって、時間的な繋がりを持たせることができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：入力画像（5×5）に対して、フィルター（3×3）で畳み込んだ時の出力画像のサイズを答えよ。（ストライド2、パッディング1）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力画像サイズの計算式は、((5+2×1-3)/2)+1=6。よって、出力画像サイズは、3×3。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：RNNのネットワークがもつ3つの重みのうち、中間層を定義する重み、中間層から出力を定義する重み、と残りもう一つの重みについて説明せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ある時点の中間層から次の時点の中間層を定義する重み。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：連鎖律の原理を使い、dx/dxを求めよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dz/dx = (dz/dt)・(dt/dx) = (2t)・(1)=2(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：y1を数式で示せ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s1 =g( Wout・(Win・x1 + W・s0 + b) + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習チャレンジ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（2）、leftとrightの特徴量を維持したまま重みをかける必要がある。そのため、処理としてはleftとrightをつなげて重みをかける演算を行う（2）が正解となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### コード演習問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ソースコードの演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バイナリ加算を行うソースコードを用いて、学習率を変更し、学習曲線の損失について考察を行なった。元の学習率（0.1）から学習率を小さく（0.05）すると損失が下がり切らず、収束しない様子が確認できた。一方で、学習率を大きく（0.5）すると、学習の初期段階で損失が0になり、学習速度が速く傾向が確認できた。一般に学習率が大きいと発散し、最適解に収束しないことが多いが、今回のケースではその様子は確認されなかった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day3-1.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section2: LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMはRNNの一種であり、入力ゲート、忘却ゲート、入力ゲートの3つのゲートから構成される。RNNでは過去の中間層の情報をそのまま次の層の中間層の入力として使用していたが、LSTMでは過去の情報をCEC（Constant Error Carousel）で記憶し、入力ゲートと出力ゲートで処理を行うことで、記憶と処理の機能を分けている。 CECの情報をどれくらい使用するかは、忘却ゲートにて調整を行っている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：シグモイド関数を微分したときの、入力値0の時の最大値。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（2）0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：どのゲートが作用するか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "影響を及ぼさない過去の情報は、CECの忘却ゲートのおける重みを調整することによって制御できる。したがって、作用するゲートは忘却ゲート。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習チャレンジ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；(2)、閾値と勾配ノルムの比が1より小さくなったとき（勾配ノルムが閾値を超えたとき）、ノルムを使って勾配を正規化するため、（2）が正解となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習チャレンジ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（3）、入力ゲートに対して重みをかけた値と忘却ゲートに対して重みかけた値が出力ゲートの値となる。また、忘却ゲートは前回のCECの状態を反映していることから、答えは（3）となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section3: GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "従来のLSTMはパラメータ数多く、計算負荷が大きいことが課題であった。そこで、GRUではパラメータを大幅に削減することで計算負荷を小さくし、同等以上の精度を得られる構造を持っている。入力層から出力層までの間に、リセットゲートと更新ゲートを持ち、隠れ層にこれまでの計算状態を保持している。リセットゲートは隠れ層の状態をどのように保持するかを制御しており、更新ゲートは前回の記憶と今回の記憶をどの程度使用するかを制御する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：LSTMと CECが抱える課題について簡潔に述べよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMはパラメータ数が膨大で、計算負荷が大きことが課題である。 CECは学習機能を持っておらず、入力ゲート、忘却ゲート、出力ゲートを組み合わせた複雑な構造を持つため、計算量が多いことが課題である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：LSTMとGRUの違いを簡潔に述べよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMは入力ゲート、忘却ゲート、出力ゲートをもち、また CECで過去の情報を記憶している。一方で、GRUはリセットゲート、更新ゲートをもつが、 CECはもたない。LSTMはパラメータ数が膨大で計算負荷が大きいのに対して、GRUは計算負荷が小さく、処理速度が速いことが特徴である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習チャレンジ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（4）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section4: 双方向RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双方向RNNは、過去の情報だけでなく、未来の情報も加味して学習を行い、精度を向上させるモデルである。特に、自然言語処理の分野において使用されることが多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習チャレンジ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（4）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section5: Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seqは、 EncoderとDecoderの二つのネットワークが組み合わさってできるネットワークである。特に、自然言語分野に用いられる。Encoderは、単語の情報をRNNに入力し、隠れ層に時系列の情報を蓄積していき、最終的に一つの文脈ベクトルを出力する。Decoderは、Encoder RNNがもつ情報をもとに、トークンを作成する。会話に例えると、Encoderは意味を解釈する役割をもち、Decoderは解釈した意味に基づいて考えを発信する役割をもつ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：seq2seqについて説明しているものを選べ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（2）、Seq2Seqは、Encoder-Decoderの構造を持ち、Encoder部分でRNNによって保持した情報をもとに、Decoder部分でトークンを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習チャレンジ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（1）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：seq2seqとHRED、HREDとVHREDの違いを簡潔に述べよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seqは、ある文章を解釈しトークンを作成する。このとき、文章全体の文脈の情報は加味されておらず、一問一答になる。一方で、HREDは、Seq2Seqに加え、RNNによって文脈間での情報を保持することができるため、文脈を考慮した返答が可能になる。VHREDは、HREDにVAEの潜在変数を用いることで、返答の情報量にバリエーションを持たせることを可能にしている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：VAEに関する説明文中の空欄に当てはまる言葉を答えよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自己符号化器の潜在変数に、確率分布（標準正規分布；平均0、標準偏差1）を導入したもの。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section6: Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vecは、文章中に含まれる単語をone-hotベクトル化し、Embedding表現を用いることで似た単語同士を似たベクトルに変換する。このとき、元々の次元数から大幅に次元数を削減したベクトルを作成することによって、大規模データの分散学習を現実的な計算速度かつメモリ量で実現可能にした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section7: Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seqでは、文章中の単語数が多い場合も少ない場合も同じ固定次元ベクトルの中に入力する必要があり、文章が長くなるほど内部表現の次元も大きくなっていくことが課題であった。そこで、Attention Mechanismでは、入力と出力の情報をもとに、単語同士の関連を学習し、不要な単語を削除することが可能になった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：RNNとword2vec、seq2seqとAttentionの違いを簡潔に述べよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " RNNは時系列データを処理するのに適したネットワークであり、word2vecは単語の分散表現ベクトルを取得するための手法である。また、seq2seqはEncoder-Decoder構造によって一つの時系列データから別の時系列データを作成する手法であり、Attention Mechanismは時系列データの中身に対して、重みを付与する手法である。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
