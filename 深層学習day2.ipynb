{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section1: 勾配消失問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差逆伝播法では出力層側から入力層側に進むにつれて、勾配がどんどん小さくなっていく。そのため、パラメータの更新を行なっても入力層側のパラメータの値はほとんど変化せず、データの収束が生じなくなる問題（勾配消失問題）が生じる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配消失問題に対する解決方法として、①活性化関数の選択（ReLU関数などを使用）、②重みの初期値の設定（Xavier、Heなどを使用）、③バッチ正規化、などの手法がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：連鎖律の原理を使い、dz/dxを求めよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day2-1.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものを選択肢から選べ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day2-2.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day2-3.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の結果より、シグモイド関数の微分は最大値0.25をとる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：重みの初期値を0に設定すると、どのような問題が発生するか。簡潔に説明せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全ての重みの更新が均一に行われるため、学習率が向上しない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：一般的に考えられるバッチ正規化の効果を2点挙げよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "②中間層での重みの更新が安定化し学習がスムーズに進むため、結果として計算の高速化につながる、②ミニバッチごとに正規化を行うため、学習データのばらつきを抑えることができ、過学習抑制につながる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例題チャレンジの答え；（1）、訓練データの特徴量、正解ラベルについて、バッチサイズ分のデータを抜き出す処理を行なっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 考察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差逆伝播法は、解析的に各重みパラメータを更新することができる方法である。一方で、勾配計算に連鎖律を使用するため、入力層側に進むにつれて勾配が小さくなることが課題であった。現在、深層学習の活性化としてよく使用されるReLU関数は、勾配消失問題に対して有効にはたらくことが期待できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section2: 学習率最適化手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習率は重みパラメータの更新量を示し、学習においてあらかじめ設定しておく必要があるパラメータである。学習率が大きい場合、最適解にたどり着かず発散してしまい、学習率が小さい場合、収束に時間がかかってしまうため、効率的な学習のためには適切な値を設定する必要がある。学習率の最適化手法には、モメンタム、AdaGrad、RMSProp、Adamなどの手法がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：モメンタム、AdaGrad、RMSPropの特徴をそれぞれ簡潔に説明せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モメンタムは、通常の勾配降下法に加え、現在の重みに前回の重みを減算した値と慣性の積を加算する。得られる解は大域的最適解になり、また谷間についてから最適解にたどり着くまでの時間が早いことが特徴である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGradは、誤差をパラメータで微分したものと再定義した学習率との積を減算する。勾配の緩やかな斜面に対しては、最適解に近づけることが特徴である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " RMSPropは、誤差をパラメータで微分したものと再定義した学習率の積を減算する。得られる解は大域的最適解になり、またハイパーパラメータの調整が不要であるケースが多いことが特徴である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ソースコードの演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適化手法Adamで実行した結果に対して、学習率、活性化関数、重みの初期化方法、バッチ正規化について変更を行い、学習曲線への影響について検証を行なった。まず、元の値（=0.01）から学習率を大きく（=0.1）場合、学習の初期段階から精度は横ばいとなり、学習が上手く進まないことがわかった。一方、学習率を小さくした（=0.001）した場合、精度は上昇し続けているが学習は完了しておらず、学習に時間を要することがわかった。次に、活性化関数をReLUに、重みの初期化方法をXavierにした場合、学習の初期段階で最適解に達しており、学習速度が速くなることがわかった。最後に、バッチ正規化を使用した場合、学習速度が速くなり、学習が安定化する傾向が確認できた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day2-4.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 考察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常の勾配降下法では、誤差をパラメータで微分したものと学習率の積を減算することによって更新が行われるが、学習率を大きく設定してしまうと最適解にたどり着けなくなるため、学習の過程で学習率を調節する必要がある。今回示した最適化手法は、過去の勾配の値を用いて更新量を決める方法や勾配の大きさに応じて学習率を調整するなどの方法を用いることで、学習速度を落とさずに大域的最適解にたどり着けるような工夫がなされている方法である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section3: 過学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深層学習において、パラメータの更新回数が増えるにつれて訓練データの学習誤差は小さくなるが、テストデータの誤差はある時点から小さくならなくなることがある。このとき、学習モデルは訓練データに対してオーバーフィッティングしており、過学習している。過学習を抑制する方法として、L1正則化またはL2正則化、ドロップアウト法などが挙げられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：リッジ回帰の特徴として正しいものを選択しなさい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（a）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：L1正則化を表しているグラフはどちらか答えよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；右、L1正則化は、正則化項としてパラメータの絶対値の総和を追加するため、2次元上に可視化すると四角形で表される。パラメータのいくつかが0になるため、スパースな推定になるのが特徴である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例題チャレンジ （L2正則化パラメータ）の答え；（4）、L2ノルム（||param^2||）の勾配は2×paramとなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例題チャレンジ （L1正則化パラメータ）の答え；（3）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例題チャレンジ （データ集合の拡張）の答え；（4）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ソースコードの演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "過学習を抑制する手法である正則化とドロップアウトについて、正則化の度合いを決めるλとドロップアウト率の精度への影響について検証を行なった。まず、ソースコード内のL1正則化項のλを大きくした場合、正則化部分の影響が大きくなってしまい、学習が上手く進んでいないことがわかった。一方で、λの値を小さくした場合、訓練データの精度が1になり、過学習が起きていることがわかった。次に、ドロップアウト率を15％から30％にすると学習が上手く進まなくなるが、最適化手法を変更することで、学習速度が速くなり、学習精度も向上することが確認できた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day2-5.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 考察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "過学習の原因は、ある重みの値のみが大きくなり、訓練データにオーバーフィッティングしてしまうためである。正則化は、パラメータのお重みが大きくなりすぎないように制御しており、過学習を抑制している。また、ドロップアウト法は学習に使用するノードをランダムに削除することにより、過学習を抑えることができる。深層学習はパラメータ数が膨大であり、更新の回数が増えるにつれて訓練データによりフィットした重みになってしまうため、ここで示したような学習方法によって過学習を抑える試みが重要だと考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section4: 畳み込みニューラルネットワークの概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込み層では、入力画像に対してフィルターを用いて画像を畳み込んでいく。演算過程では、フィルターサイズ、パディング、ストライド、チャネル数によって出力される画像サイズが決まる。画像データに畳み込みを行うことで、縦、横、チャネルの3次元情報をそのまま学習でき、画像の空間情報を次の層に伝えることができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト：入力画像（6×6）をフィルター（2×2）で畳み込んだ時の出力画像のサイズを答えよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day2-6.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ソースコードの演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＜畳み込み層＞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4×4の入力画像に対してストライド1で3×3のフィルターを適用すると、2×2の出力画像が得られる。そのため、得られる2次元配列は4×9の行列となる。なお、今回は2枚の画像に対して適用しているため、8×9の行列となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day2-7.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パッディングを1することで、6×6の入力画像になる。そのため、3×3のフィルターをストライド1で適用すると、4×4の出力画像が得られる。そのため、得られる2次元配列は16×9の行列となる。なお、今回は2枚の画像に対して適用しているため、32×9の行列となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day2-8.png\" width=100%>\n",
    "<img src = \"実装演習_深層学習day2-9.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＜プーリング層＞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4×4の入力画像に対して、2×2のフィルターを用いてストライド2でMaxPoolingをすると、2×2の出力画像が得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day2-10.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section5: 最新のCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期のCNNの代表的なモデルにAlexNetがある。AlexNetは物体画像データセットImageNetを用いたコンペにおいて好成績を残したモデルである。入力層は3×224×224の画像であり、3層の畳み込み層、2層のMaxプーリング層、3層の全結合層を用いて、最終的に1000個の値を出力する。活性化関数にはReLU関数を用いており、また過学習を抑制する目的で、局所正規化、全結合層の出力にドロップアウトを使用している。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
