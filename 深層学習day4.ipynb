{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section1: 強化学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習では、データに含まれるパターンを見つけ出すことを目的する教師ありなし学習とは異なり、優れた方策（行動指針）を導き出すための手法である。学習初期は不完全な情報をもとにランダムに学習を進めていくが、学習が進むと過去の情報をもとに学習を進めていく。強化学習では、主に方策関数と価値関数（行動価値関数）について学習を行う。方策関数は方策ベースの強化学習において、ある状態でどのような行動をとるかの確率を与え、価値関数は状態を価値を組み合わせた価値に注目する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section2: AlphaGo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaGoでは、囲碁に対して教師あり学習と強化学習を組み合わせた学習手法である。強化学習に用いる方策関数には盤面の19×19マスの着手予想確率を算出するPolycyNetを使用し、価値関数には現局面の勝率を出力するValueNetを用いる。学習ステップにおいて、PolicyNetはニューラルネットワークを用いているため、計算速度が遅いことが課題であった。そこで、RollOutPolicyでは線形関数を用いることで、計算速度を1000倍高速化することを可能にした。AlphaGoの学習ステップ1では、過去の棋譜データを用いた教師あり学習によってPolicyNetとRollOutPolicyを学習させる。学習ステップ2、3では、強化学習によってPolicyNetとValueNetの学習を行う。ValuNetの更新手法には、モンテカルロ木探索を用いて更新を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaGoには、AlphaGo LeeとAlphaGo Zeroがある。後発のAlphaGo Zeroでは、①強化学習のみでモデルを構築、②人的要素を排除し、入力特徴量は石の配置のみ、③PolicyNetとValueNetを一つに統合、④Residual Netを導入、⑤モンテカルロ木探索をなくす、の変更を行なった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResidualNetは、ネットワークにショートカット構造を追加することで、勾配消失を抑えることが期待できる。また、層数の異なるネットワークが作成されることで、アンサンブル効果が得られると考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section3: 軽量化・高速化技術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高速化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ並列化には、同期型と非同期型の2種類がある。同期型は並列化した子モデルの勾配の平均を算出し、親モデルのパラメータを更新を行う。非同期型は子モデルごとにパラメータ更新を行い、学習終了後はパラメータサーバーにモデルをPushする。次に学習を行う際は、パラメータサーバーからPopしたモデルに対して学習を行なっていく。非同期型は全ての子モデルの計算が終了するのを待つ必要がないため学習速度は速いが、学習が不安定になりやすい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル並列化は、親モデルを分割してそれぞれのモデルを学習させ、全ての学習が終わった後で、一つのモデルに復元する。モデル並列化は、モデルのパラメータ数が多いほど計算速度の効率も向上する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深層学習は計算量が多い一方で、計算内容は単純な行列演算がメインである。そのため、計算速度を向上させるためには、複雑な処理を得意とするCPUではなく、簡単な並列計算を得意とするGPUを用いることが有用だと考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 軽量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量子化は、通常のパラメータの数値表現で使用される64bit浮動小数点を32bitなどの下位精度に落とすことで、メモリと演算処理の削減を行うことである。量子化は、省メモリ化、計算速度の高速化が利点である一方で、精度の低下が懸念される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蒸留は、学習済みの精度の高いモデルから軽量な生徒モデルを作成することで、効率よく学習を行うことが可能である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "プルーニングは、モデルの精度に寄与が少ないニューロンを削減することでし、モデルの軽量化・高速化を見込む手法である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section4: 応用モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MobileNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNetsは畳み込みニューラルネットワークの１つであり、通常のディープラーニングモデルで使用される畳み込み演算について、Depthwise ConvolutionとPointwise Convolutionを用いることで計算量を削減し、軽量化を行なったネットワークである。Depthwise Convolutionはフィルター数を1に固定し、複数チャネルの入力画像に対して1チャネルごとに畳み込みを行う。PointwiseConvolutionはフィルターサイズを1に固定し、畳み込みを行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（い）H × W × C × K × K、（う）H × W × C × M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Densenetは、畳み込みニューラルネットワークの１つであり、DenseBlockを用いて前の層で計算した出力を次の入力に足し合わせて計算を行う。例えばkチャネル出力する畳み込みを行なった場合、1層通過するごとにkチャネルずつチャネル数が増えていく。DenseBlockの間にはTransition layerと呼ばれる層があり、チャネル数を変更しダウンサンプリングを行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNetと似た構造をもつネットワークにResNetがある。ResNetはResidualBlockによって前1層の入力を後の層への入力として用いるが、DenseNetはDenseBlockによって前方の各層からの出力全てが後方への入力として用いられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WaveNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "時系列データに対してDilated convolutionを適用する。Dilated convolutionの特徴として、層が深くなるにつれて畳み込むリンクを離すことで、受容野を増やすことができる利点がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え；（あ）Dilated convolution、（い）パラメータ数に対する受容野が広い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section5: Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2seqは系列を入出力にとり、内部はRNNを用いたEncoderとDecoderのモデルである。Seq2seqは全て微分可能なため、Decoderの出力側に正解を与えることで、end2endで教師あり学習が可能になる。Seq2seqはRNNによって連鎖的に情報が引き継がれていくため、誤差が大きくなってしまうことが課題に挙げられる。そこで、訓練時に正解ラベルを直接Decoderの入力に用いるTeacher Forcingが使用される。一方で、Teacher Forcingは訓練時は収束したモデルであっても、テスト時には収束しないケースがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformerは大きくEncoder、Decoderに分かれたネットワーク構造を持ち、RNNを使用せずAttentionのみを使用する。Attentionは辞書オブジェクトを示し、クエリーに一致するkeyを索引し、対応するvalueを取り出す操作とみなすことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day4-1.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day4-2.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day4-3.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day4-4.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day4-5.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"実装演習_深層学習day4-6.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section6: 物体検知・セグメンテーション"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物体認識には大きく分けて、分類、物体検知、セマンティックセグメンテーション、インスタンスセグメンテーションがある。分類では物体が映っているかいないかに主眼を置いていたが、物体検知、セグメンテーションでは物体がどこに映っているかに主眼をおく。分類問題はConfusion Matrixを用いて評価を行うが、物体検知・セグメンテーションはそれに加え、IoU(Intersection over Union)を用いて評価を行う。IoUは、IoU=TP/(TP + FP + FN)として表せる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
